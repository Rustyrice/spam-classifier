{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Please ensure you carefully read all instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please ask the teaching team well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "**You should also submit a readme file (.txt or .md) explaining your implementation, any decisions or extensions you made, and what parameter values you used.** This explanation should demonstrate what you have done and that you have done it on your own, e.g., you understand methods that you have used. Failure to submit a readme file, or if the readme file does not contain enough information to demonstrate what you have done and that you actually understand what you have done may result in getting zero marks for this assignment.\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. These assignments are designed to help improve your understanding first and foremost – the process of doing the assignment is part of *learning*. They are also used to assess your ability, and so you must uphold academic integrity. Submitting plagiarised work risks your entire place on your degree.\n",
    "\n",
    "**This assignment is worth 20% of your total mark for the unit. The deadline for this assignment is 12 May 2023 at 8 pm UK time.** Please ask for help if you are struggling. Your submission should include:\n",
    "\n",
    "* this notebook filled with your code\n",
    "* readme file (.txt or .md)\n",
    "* any additional .py files that you may wish to use (remember to properly import them in this notebook)\n",
    "\n",
    "## Getting Started\n",
    "This assignment contains 2 parts. The first one is **compulsory** and it is worth 70% of your mark for this assignment. The second part is **optional** if you would like to get a higher (over 70%) mark for this assignment. \n",
    "\n",
    "This assignment is devoted to supervised machine learning. In the first part of the assignment you are asked to write a classifier to detect any spam email messages. In the second part of the assignment you are asked to work on feature design for a classifier to classify hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 (compulsory, worth 70% of the mark for this assignment) - Spam filtering\n",
    "\n",
    "Spam refers to unwanted email, often in the form of advertisements. In the literature, an email that is **not** spam is called *ham*. Most email providers offer automatic spam filtering, where spam emails will be moved to a separate inbox based on their contents. Of course this requires being able to scan an email and determine whether it is spam or ham, a classification problem. This is the subject of this part of the assignment.\n",
    "\n",
    "You will need to write a supervised learning based classifier to determine whether a given email is spam or ham. You must write and submit the code in this notebook. The training data is provided for you. You may use any classification method. Marks will be awarded primarily based on the accuracy of your classifier on unseen test data, but there are also marks for estimating how accurate you think your classifier will be.\n",
    "\n",
    "\n",
    "## Choice of Algorithm\n",
    "While the classification method is a completely free choice, the assignment folder includes [a separate notebook file](data/naivebayes.ipynb) which can help you implement a Naïve Bayes solution. If you do use this notebook, you are still responsible for porting your code into *this* notebook for submission. A good implementation should give a high  enough accuracy to get a good grade.\n",
    "\n",
    "You could also consider a k-nearest neighbour algorithm, but this may be less accurate. Logistic regression is another option that you may wish to consider.\n",
    "\n",
    "If you are looking to go beyond the scope of the unit, you might be interested in building something more advanced, like an artificial neural network. This is possible just using `numpy`, but will require significant self-directed learning. *Extensions like this are left unguided and are not factored into the unit workload estimates.*\n",
    "\n",
    "**Note:** you may use helper functions in libraries like `numpy` or `scipy`, but you **must not** import code which builds entire models for you. This includes but is not limited to use of libraries like `scikit-learn`, `tensorflow`, or `pytorch` – there will be plenty of opportunities for these libraries in later units. The point of this assignment is to understand code of the actual algorithm yourself. ***If you are in any doubt about any particular library or function please ask the teaching team.*** Submissions which ignore this will receive penalties or even zero marks.\n",
    "\n",
    "If you choose to implement more than one algorithm, please feel free to include your code in a separate file and talk about it the readme file, but only the code in this notebook will be used in the automated testing and it will form your mark. You may get extra points for the additional algorithm, but it is not guaranteed. Therefore, if of any doubt, it is better to spend your time focusing on making one algorithm working to its best performance rather than to implement an additional algorithm.\n",
    "\n",
    "## Training Data - spam filtering\n",
    "The training data is described below and has 1000 rows. There is also a 500 row set of test data. These are functionally identical to the training data, they are just in a separate csv file to encourage you to split out your training and test data. You will be assessed on the additional hidden test data. You should consider how to best make use of all available data without overfitting, and to help produce an unbiased estimate for your classifier's accuracy.\n",
    "\n",
    "The cell below loads the training data into a variable called `training_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or ham `0`. The remaining 54 columns are _features_ that you will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned, there is also a 500-row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam testing data set: (500, 55)\n",
      "[[1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filtering classification\n",
    "**Write all of the code for your classifier below this cell.** \n",
    "\n",
    "There is some very rough skeleton code in the cell directly below. You may insert more cells below this if you wish, but you **must not** duplicate any cells as this can break the grading script.\n",
    "\n",
    "### Submission Requirements\n",
    "Your code must provide a variable with the name `spam_classifier`. This object must have a method called `predict` which takes input data and returns class predictions. The input will be a single $n \\times 54$ numpy array, your classifier should return a numpy array of length $n$ with classifications. There is a demo in the cell below, and a test you can run before submitting to check your code is working correctly.\n",
    "\n",
    "Your code must run on our test machine in under 30 seconds. If you wish to train a more complicated model (e.g. neural network) which will take longer, you are welcome to save the model's weights as a file and then load these in the cell below so we can test it. You must include the code which computes the original weights, but this must not run when we run the notebook – comment out the code which actually executes the routine and make sure it is clear what we need to change to get it to run. \n",
    "\n",
    "Remember that we will be testing your final classifier on additional hidden data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def estimate_log_class_priors(self, labels):\n",
    "        \"\"\"\n",
    "        Given a data set with binary response variable (0s and 1s), \n",
    "        calculate the logarithm of the empirical class priors,\n",
    "        that is, the logarithm of the proportions of 0s and 1s:\n",
    "            log(p(C=0)) and log(p(C=1))\n",
    "\n",
    "        :param data: a numpy array of length n_samples\n",
    "                     that contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "        :return log_class_priors: a numpy array of length two\n",
    "        \"\"\"\n",
    "        # determine number of samples in data\n",
    "        n_samples = len(labels)\n",
    "        # count number of 0s and 1s in data\n",
    "        count_0 = np.sum(labels == 0)\n",
    "        count_1 = np.sum(labels == 1)\n",
    "        # calculate logarithm of empirical class priors\n",
    "        log_prior_0 = np.log(count_0 / n_samples)\n",
    "        log_prior_1 = np.log(count_1 / n_samples)\n",
    "        log_class_priors = np.array([log_prior_0, log_prior_1])\n",
    "        return log_class_priors\n",
    "    \n",
    "    def estimate_log_class_conditional_likelihoods(self, input_data, labels):\n",
    "        \"\"\"\n",
    "        Given input_data of binary features (words) and labels \n",
    "        (binary response variable (0s and 1s)), calculate the logarithm \n",
    "        of the empirical class-conditional likelihoods, that is,\n",
    "        log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "        Assume a multinomial feature distribution and use Laplace smoothing\n",
    "        if alpha > 0.\n",
    "\n",
    "        :param input_data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "                           contains binary features (words)\n",
    "        :param labels: a numpy array of length n_samples \n",
    "                       contains response variable\n",
    "\n",
    "        :return theta:\n",
    "            a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "            logarithm of the probability of feature i appearing in a sample belonging \n",
    "            to class j.\n",
    "        \"\"\"\n",
    "        # get number of samples and features in input data\n",
    "        n_samples, n_features = input_data.shape\n",
    "        # count number of labels that are 0 and 1\n",
    "        count_0 = np.sum(labels == 0)\n",
    "        count_1 = np.sum(labels == 1)\n",
    "        \n",
    "        # initialise theta\n",
    "        theta = np.zeros((2, n_features))\n",
    "\n",
    "        # compute class-conditional likelihoods\n",
    "        for j in range(2):\n",
    "            # get indices of input_data where label is j\n",
    "            indices = np.where(labels == j)[0]\n",
    "            # calculate probability of each feature given the class j\n",
    "            # add self.alpha to avoid probabilities of zero\n",
    "            numerator = np.sum(input_data[indices, :], axis=0) + self.alpha\n",
    "            denominator = np.sum(input_data[indices, :]) + self.alpha * n_features\n",
    "        \n",
    "            theta[j, :] = numerator / denominator\n",
    "            \n",
    "        # convert probabilities to logarithms\n",
    "        theta = np.log(theta)\n",
    "        return theta\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        \"\"\"\n",
    "        Given input_data of binary features (words) and labels \n",
    "        (binary response variable (0s and 1s)), calculate \n",
    "        * the logarithm of the empirical class priors, that is, \n",
    "          the logarithm of the proportions of 0s and 1s:\n",
    "            log(p(C=0)) and log(p(C=1))\n",
    "        * the logarithm of the empirical class-conditional likelihoods, \n",
    "          that is, log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "        Assume a multinomial feature distribution and use Laplace smoothing\n",
    "        if alpha > 0.\n",
    "\n",
    "        :param input_data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "        :param labels: a one-dimensional numpy-array with shape = [n_samples,]\n",
    "        :param alpha: a Laplace smoothing parameter\n",
    "\n",
    "        :return \n",
    "            log_class_priors: a numpy array of length two\n",
    "\n",
    "            theta:\n",
    "            a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "            logarithm of the probability of feature i appearing in a sample belonging \n",
    "            to class j.\n",
    "        \"\"\"\n",
    "        self.log_class_priors = self.estimate_log_class_priors(y)\n",
    "        self.log_class_conditional_likelihoods = self.estimate_log_class_conditional_likelihoods(x, y)\n",
    "        \n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Given a new data set with binary features, predict the corresponding\n",
    "        response for each instance (row) of the new_data set.\n",
    "\n",
    "        :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "        :param log_class_priors: a numpy array of length 2.\n",
    "        :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "            theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "            in a sample belonging to class j.\n",
    "        :return class_predictions: a numpy array containing the class predictions for each row\n",
    "            of new_data.\n",
    "        \"\"\"\n",
    "        # calculate the log of prior probability for each class\n",
    "        log_prior_0 = self.log_class_priors[0]\n",
    "        log_prior_1 = self.log_class_priors[1]\n",
    "\n",
    "        # calculate unnormalised posterior probabilities for each row and each class\n",
    "        posterior_0 = np.exp(log_prior_0 + np.sum(self.log_class_conditional_likelihoods[0] * new_data, axis=1))\n",
    "        posterior_1 = np.exp(log_prior_1 + np.sum(self.log_class_conditional_likelihoods[1] * new_data, axis=1))\n",
    "        \n",
    "        # assign class with highest posterior probability as the predicted class using argmax\n",
    "        class_predictions = np.argmax([posterior_0, posterior_1], axis=0)\n",
    "\n",
    "        return class_predictions\n",
    "\n",
    "def create_classifier():\n",
    "    spam_classifier = MyClassifier(alpha=1.0)\n",
    "    # load training data from CSV file into numpy array\n",
    "    train_data = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\", skiprows=1)\n",
    "    # separate features (x_train) and target labels (y_train) from training data\n",
    "    x_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "    # train classifier using training data\n",
    "    spam_classifier.train(x_train, y_train)\n",
    "    return spam_classifier\n",
    "\n",
    "spam_classifier = create_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Estimate\n",
    "In the cell below there is a function called `my_accuracy_estimate()` which returns `0.5`. Before you submit the assignment, write your best guess for the accuracy of your classifier into this function, as a percentage between `0` and `1`. So if you think you will get 80% of inputs correct, return the value `0.8`. This will form a small part of the marking criteria for the assignment, to encourage you to test your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy_estimate():\n",
    "    return 0.898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write all of the code for part 1 of the assignment above this cell.**\n",
    "\n",
    "### Testing Details\n",
    "Your classifier will be tested against some hidden data from the same source as the original. The accuracy (percentage of classifications correct) will be calculated, then benchmarked against common methods. At the very high end of the grading scale, your accuracy will also be compared to the best submissions from other students (in your own cohort and others!). Your estimate from the cell above will also factor in, and you will be rewarded for being close to your actual accuracy (overestimates and underestimates will be treated the same).\n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "The original skeleton code above classifies every row as ham, but once you have written your own classifier you can run this cell again to test it. So long as your code sets up a variable called `spam_classifier` with a method called `predict`, the test code will be able to run. \n",
    "\n",
    "Of course, you may wish to test your classifier in additional ways, but you *must* ensure this version still runs before submitting.\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "\n",
    "    predictions = spam_classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (optional, to get more than 70% of the mark for this assignment) - Feature engineering for digit classification\n",
    "\n",
    "Optical character recognition ([OCR](https://en.wikipedia.org/wiki/Optical_character_recognition)) is the task of extracting text from image sources. The dataset on which you will <font color='red'>~~run your classifier~~ develop new features</font> is a collection of handwritten numerical digits (0-9). This is a subsample of the famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology) dataset, which serves as a popular playground in machine learning research. OCR is a very commercially useful technology, and some existing systems can achieve over 99% classification accuracy.\n",
    "\n",
    "<font color='red'>~~You will need to train a classifier~~ The classification task is</font> to recognise a given image of a handwritten digit as one of 10 classes $\\{0, 1, \\ldots, 9\\}$ that corresponds to the written digit. <font color='red'> ~~You may use any classification method including the same classification method you use for part 1 for spam filtering. If your `MyClassifier` class can generalise to an arbitrary number of input features and artibtary number of class labels, you may use the same `MyClassifier` for both parts. You will need to have **two different instances** (`spam_classifier = MyClassifier()` and `digit_classifier = MyClassifier()`) for part 1 and part 2 of the assignment.~~</font> The main task of part 2 is to improve the classification accuracy via feature engineering (see below). Your mark for part 2 will not directly depend on your choice of classification method and its initial accuracy. \n",
    "\n",
    "Building classifiers is only a small part of getting a good system working for a task. Indeed, the main difference between a good classification system and a bad one is usually not the classifier itself (e.g., perceptron vs. naïve Bayes), but rather the quality of the features used. \n",
    "\n",
    "For digit classification we start with the simplest possible features: for each pixel we use a binary variable, which is set to 1 if a pixel is on (its' colour is not black), or it is set to 0 if a pixel is off (its' colour is black). These are the basic features with which you will need to compare. \n",
    "\n",
    "To improve the accuracy of your classifier, you will need to extract more useful features from the data by completing the `BetterFeatureExtractor` function below. \n",
    "\n",
    "*Hint*: You are free to choose how to approach this task. This is just a possible suggestion. When analyzing your classifiers' results on the base features, you should look at some of your errors and look for characteristics of the input that would give the classifier useful information about the label. For instance in the digit data, consider the number of separate, connected regions of white pixels, which varies by digit type. Digits 1, 2, 3, 5, 7 tend to have one contiguous region of white space while the loops in digits 6, 8, 9 create more. The number of white regions in digit 4 depends on the writer. This is an example of a feature that is not directly available to the classifier from the per-pixel information. If your feature extractor adds new features that encode these properties, the classifier will be able to exploit them. Note that some features may require non-trivial computation to extract.\n",
    "\n",
    "Add new **binary** features for the digit dataset in the `BetterFeatureExtractor` function below. Note that you can encode a categorical feature which takes N values [1,2,...,N] by using N binary features, of which only one is on at the time, to indicate which of the N possibilities you have. In theory, features aren't conditionally independent as naïve Bayes requires, but it can still work well in practice. Note that you may add new feature(s) to the set of basic features, or use only new feature(s) instead of the basic features.\n",
    "\n",
    "The marks for this part of the assignment are given based on the improvement you achieve by using new feature(s) in comparison to the base features on additional hidden data. You will get some marks for implementing new feature(s) which yield any improvement at all. Further marks are given based on how large is the gain in accuracy you achieve. Note that the marks don't depend on the initial accuracy achieved based on the basic features (as long as it is higher than 10% (random guess for a 10-class classification problem)). \n",
    "\n",
    "<font color='red'>During your testing, you would need to implement a classification method yourself. For assessment we will use our own implementation of the naïve Bayes classifier, i.e., **only your implementation of the `BetterFeatureExtractor` function would affect your mark for part 2 of the coursework**. During your testing you are advised to use the same `MyClassifier` class. You would need to ensure that it can generalise to an arbitrary number of input features and artibtary number of class labels, then you may use the same `MyClassifier` for both parts of the assignment. You are, of course, free to implement a different classification method for part 2, for example, if you implement different than naïve Bayes classifier for part 1 and wish to be as close as possible to the assessment test during your testing. Please note, however, your mark is not directly affected if you implement an additional classification method for part 2. Only implementation of the `BetterFeatureExtractor` function would affect your mark. That is, we will first train our implementation of the naïve Bayes on the basic features and compute the accuracy it gets on the hidden test data. We then train another instance of our implementation of the naïve Bayes on the new features provided by your `BetterFeatureExtractor` function and compute the accuracy on the hidden test data. Then your mark will depend on the difference between these two accuracies. </font>\n",
    "\n",
    "\n",
    "## Training Data - digit classification\n",
    "You are given training and test datasets for digit classification. The training data is described below and has 300 data point. There are also 300 data points of test data. As for spam filtering data the test data is functionally identical to the training data.\n",
    "\n",
    "The cell below loads the input training data into a variable called `training_digit_input` and their labels into a variable called `training_digit_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the digit input training data set: (300, 28, 28)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 127 255\n",
      "  207   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  13  89 244 253\n",
      "  236  44   0   0   0   0   0   0   0   0]\n",
      " [  0   0  38  91 184  90  38   0   0   0   0  32  70 142 203 252 252 253\n",
      "  252  69   0   0   0   0   0   0   0   0]\n",
      " [  0   0 138 252 252 252 232 208 207 207 207 228 253 252 252 252 252 253\n",
      "  252  69   0   0   0   0   0   0   0   0]\n",
      " [  0   0  23 158 252 252 252 253 252 252 252 252 253 200 136  22 128 253\n",
      "  231  37   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  53 116  42   0  43  11   0   0   0   0 231 255\n",
      "  207   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 230 253\n",
      "  206   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 230 253\n",
      "  206   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  34  93   0   0   0  76 248 253\n",
      "  206   0  76  51   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  32 218 252 231 230 230 238 252 253\n",
      "  248 230 248  85   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 185 253 253 255 253 253 253 253 255\n",
      "  249 230  42   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  99 206 206 207 206 219 252 252 218\n",
      "   75   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 253 253 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  70 252 252 116\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "Shape of the digit label training data set: (300,)\n",
      "[7 2 4 6 5 6 8 2 5 7 0 2 3 2 5 1 3 3 9 6 1 7 0 0 7 4 2 2 5 2 7 6 8 2 0 7 8\n",
      " 5 0 3 4 3 6 7 8 2 0 8 7 3 9 0 4 3 9 0 8 0 5 6 4 4 1 8 4 0 3 6 1 4 3 8 5 8\n",
      " 2 8 3 1 6 4 4 9 7 1 8 1 3 1 6 0 6 9 7 9 9 1 7 6 5 7 3 6 6 8 6 7 9 6 0 0 8\n",
      " 4 1 1 0 5 7 4 8 2 5 2 4 3 7 5 5 9 2 6 8 7 9 2 5 1 8 8 4 6 7 6 6 9 7 6 8 8\n",
      " 9 5 2 7 8 0 4 9 3 4 8 5 1 1 2 6 2 2 8 9 4 4 3 2 9 9 1 4 4 7 3 4 5 5 0 4 1\n",
      " 2 0 5 9 3 9 5 0 0 3 3 2 9 9 9 1 1 3 1 9 6 3 7 3 9 4 5 0 1 8 5 0 1 1 9 5 6\n",
      " 0 6 8 1 3 4 9 7 2 1 9 3 5 6 3 0 2 7 4 2 1 0 2 0 8 1 1 1 2 4 7 8 5 0 6 5 6\n",
      " 4 6 4 3 3 0 3 2 2 7 0 9 1 7 5 3 5 7 9 5 4 7 7 9 5 8 1 8 0 2 4 5 8 3 2 8 9\n",
      " 6 7 6 0]\n"
     ]
    }
   ],
   "source": [
    "training_digit_input = np.load(\"data/training_digit_input.npy\")\n",
    "print(\"Shape of the digit input training data set:\", training_digit_input.shape)\n",
    "print(training_digit_input[0])\n",
    "training_digit_label = np.load(\"data/training_digit_label.npy\")\n",
    "print(\"Shape of the digit label training data set:\", training_digit_label.shape)\n",
    "print(training_digit_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your input training set `training_digit_input` is an array of size (300, 28, 28). Each data point (along the 0th dimension) is an image of 28x28 pixels. `training_digit_label` contains the corresponding labels for these images. It is an array of length 300. \n",
    "\n",
    "The cell below visualises the first 5 images with their labels as title of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    plt.imshow(training_digit_input[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{training_digit_label[i]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same structure of the data is used for the *test data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the digit input test data set: (300, 28, 28)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  62 131 123 236 148 148  87  43  43  14\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  27 218 252 252 252 252 253 252 252 211\n",
      "  102   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12  21  21  91  82 144 231 240 245\n",
      "  252 216  32   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 169\n",
      "  252 253 133   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  36 203 239\n",
      "  252 253 124   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 124 166 253 253 253\n",
      "  253 212  27   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 211 253 252 252 196\n",
      "  124  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 106 232 247 252  93\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  99 247 231\n",
      "   28   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 188 252\n",
      "  129   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  25 148 113 172   0   0   0 110 253\n",
      "  165   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 157 252 174  56   0   0   0 101 252\n",
      "  252   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  98 253 252  21   0   0   0   0 127 252\n",
      "  252   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 106 253 169   7   0   0   0   0 197 252\n",
      "  252   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  54 184 253  63   0   0   0  18  27 232 252\n",
      "  164   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 109 253 247  53   0   0  36 255 174 245 243\n",
      "   79   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  43 239 225  21   8 155 242 253 252 252 110\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 115 253 237 234 252 252 253 252 136  42\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 253 252 252 252 252 190 110   7   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  60 226 208 182 103   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "Shape of the digit label test data set: (300,)\n",
      "[3 4 6 4 0 1 9 8 4 2 8 5 1 9 3 6 6 1 1 4 4 0 4 3 4 4 1 4 2 4 6 0 2 2 4 6 0\n",
      " 5 7 4 1 4 7 2 1 9 8 0 8 0 3 3 9 5 3 4 5 3 8 2 6 5 1 6 5 2 8 4 4 9 7 2 1 7\n",
      " 8 0 4 9 6 0 5 8 2 9 3 5 3 3 9 5 0 3 8 7 9 9 7 0 7 0 3 2 1 9 7 7 7 7 6 3 7\n",
      " 1 8 6 8 6 6 2 9 0 7 1 5 7 2 7 9 3 8 9 7 2 8 1 2 3 8 9 1 9 2 2 1 1 7 9 6 7\n",
      " 5 3 6 1 0 8 1 6 5 7 3 8 4 0 3 6 6 0 7 6 3 1 9 8 3 2 9 9 5 3 4 9 7 9 9 8 1\n",
      " 0 5 1 1 0 4 3 1 5 7 5 3 1 8 4 6 6 5 3 6 6 0 8 2 1 2 9 4 2 7 9 5 1 5 6 5 5\n",
      " 2 8 7 0 6 3 1 0 5 9 2 8 0 6 0 0 6 0 8 4 4 9 4 5 9 1 5 0 2 4 5 8 5 7 9 8 2\n",
      " 5 2 3 4 0 2 8 8 9 0 4 8 7 6 6 8 1 0 8 3 4 2 7 0 6 5 2 7 2 0 4 3 1 5 7 3 2\n",
      " 6 7 3 5]\n"
     ]
    }
   ],
   "source": [
    "test_digit_input = np.load(\"data/test_digit_input.npy\")\n",
    "print(\"Shape of the digit input test data set:\", test_digit_input.shape)\n",
    "print(test_digit_input[0])\n",
    "test_digit_label = np.load(\"data/test_digit_label.npy\")\n",
    "print(\"Shape of the digit label test data set:\", test_digit_label.shape)\n",
    "print(test_digit_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing feature extraction the input data is flatten to have the shape (n, 784), where $784 = 28 \\times 28$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering for digit classification\n",
    "**Write all of the code for your feature extraction below this cell.**\n",
    "\n",
    "We also provide the code of the basic feature extraction for your reference. You may insert more cells below this if you wish, but you **must not** duplicate any cells as this can break the grading script.\n",
    "\n",
    "### Submission Requirements\n",
    "The code for your feature extractor should be included in the `BetterFeatureExtractor` function below. We will call this function to generate new features.\n",
    "\n",
    "<font color='red'>**If you wish to use the testing code provided below**, your code must provide a variable with the name `digit_classifier`. This object must have a method called `train` which takes input data and input labels and tune internal parameters of the classifier (if required). If your classifier does not need to use the `train` method (e.g., k-Nearest Neighbour method doesn't use explicit training), you should still have the `train` method, but you can leave it empty with the command `pass`.\n",
    "\n",
    "`digit_classifier` must also have a method called `predict` which takes input data and returns class predictions.\n",
    "\n",
    "Since you are only assessed for part 2 by your implementation of the `BetterFeatureExtractor` function, you do not have to have the `digit_classifier` variable. In this case the testing code provided below will skip some tests.\n",
    "</font>\n",
    "\n",
    "The input data will be a single $n \\times m$ numpy array, where $n$ is the number of data points and $m$ is the number of features ($m = 784$ for the basic features). Your classifier <font color='red'> for your testing</font> should support working with different number of features if your new features are of a different dimension. <font color='red'> Our implementation of the classifier that will be used for assessment does support working with different number of features, so your new features can be of any dimension.</font> The input labels will be a single numpy array of length $n$ with true classifications for the input data (for the `train` method).\n",
    "\n",
    "<font color='red'> If you wish to use the testing code provided below,</font> your classifier when calling the `predict` method should return a numpy array of length $n$ with classifications. There is a demo in the cells below, and a test you can run before submission to check that your code is working correctly.\n",
    "\n",
    "<font color='red'>~~Your code~~ Our implementation of the naïve Bayes classification method</font> must run on our test machine in under 5 minutes - both training and prediction (on given training and test data <font color='red'>with your new features</font>).\n",
    "\n",
    "Remember that we will be testing your <font color='red'>~~classifier~~ new features</font> on additional hidden data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96679e416c1f5b5ea9a692ee60b42dfd",
     "grade": false,
     "grade_id": "cell-8cc92a4b0ae1078f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def BasicFeatureExtractor(input_data):\n",
    "    \"\"\"\n",
    "    Returns basic extracted features for input_data. \n",
    "    A binary feature for each pixel: 0 if a pixel is black, 1 otherwise\n",
    "    \n",
    "    :param input_data: a 3-dimensional numpy array of the shape (n, 28, 28)\n",
    "                       input data of n images of 28x28 pixels\n",
    "    \n",
    "    :return extracted_features: a 2-dimensional numpy array of the shape (n, 784)\n",
    "                                extracted binary features \n",
    "    \"\"\"\n",
    "    # compute binary features for each pixel\n",
    "    extracted_features = (input_data > 0).astype(int)\n",
    "    \n",
    "    # flatten images of 28x28 pixels into a vector of 784 length\n",
    "    extracted_features = np.reshape(extracted_features, (extracted_features.shape[0], -1))\n",
    "    \n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BetterFeatureExtractor(input_data):\n",
    "#     \"\"\"\n",
    "#     A function for your improved feature extractor\n",
    "    \n",
    "#     :param input_data: a 3-dimensional numpy array of the shape (n, 28, 28)\n",
    "#                        input data of n images of 28x28 pixels\n",
    "    \n",
    "#     :return extracted_features: a 2-dimensional numpy array of the shape (n, m)\n",
    "#                                 extracted binary m features for n images  \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # You may not use this line below\n",
    "#     extracted_features = BasicFeatureExtractor(input_data)\n",
    "    \n",
    "#     ### YOUR CODE HERE...\n",
    "    \n",
    "#     return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example creation and training of a digit classifier. A new instance of MyClassifier class from above\n",
    "# # digit_classifier = MyClassifier(k=1)\n",
    "# digit_classifier.train(train_data=None, train_labels=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write all of the code for part 2 of the assignment above this cell.**\n",
    "\n",
    "### Testing Details\n",
    "<font color='red'>~~Your classifier~~ Our classifier</font> performance with both the basic and new features will be tested against some hidden data. The accuracy (percentage of classifications correct) will be calculated and measured whether you get any improvement by using new features in comparison to using the basic features.\n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier with both the basic and new features against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`. <font color='red'>You do not have to use these tests as we are only assessing your implementation of the `BetterFeatureExtractor` function. These tests are provided to help you testing your new features, but you can ignore these tests and use your own. </font>\n",
    "\n",
    "The original skeleton code above classifies every row as 0 (and do not depend on the input data). Moreover, the original code of BetterFeatureExtractor function just returns the basic features. However, once you have written your own classifier and your own BetterFeatureExtractor function you can run this cell again to test it. So long as your code implements BetterFeatureExtractor function and sets up a variable called `digit_classifier` with methods called `train` and `predict`, the test code will be able to run. \n",
    "\n",
    "As with the spam filtering you may wish to test your classifier in additional ways<font color='red'>. ~~, but you *must* ensure this version still runs before submitting.~~</font>\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    test_digit_input = np.load(\"data/test_digit_input.npy\")\n",
    "    test_digit_label = np.load(\"data/test_digit_label.npy\")\n",
    "    \n",
    "    # train classifier on basic features\n",
    "    training_data = BasicFeatureExtractor(training_digit_input)\n",
    "    \n",
    "    if \"digit_classifier\" not in locals():\n",
    "        print(\"There is no digit_classifier variable, so tests are unable to run\")\n",
    "    elif (not hasattr(digit_classifier, 'train')) or \\\n",
    "        (not hasattr(digit_classifier, 'predict')):\n",
    "        print(\"The variable digit_classifier does not have train and predict methods, so tests are unable to run\")\n",
    "    else:\n",
    "        digit_classifier.train(training_data, training_digit_label)\n",
    "\n",
    "        # test classifier on basic features\n",
    "        test_data = BasicFeatureExtractor(test_digit_input)\n",
    "        basic_feature_predictions = digit_classifier.predict(test_data)\n",
    "        basic_feature_accuracy = np.count_nonzero(\n",
    "            basic_feature_predictions == test_digit_label)/test_digit_label.shape[0]\n",
    "        print(f\"Accuracy on test data using basic features is: {basic_feature_accuracy}\")\n",
    "\n",
    "        # train classifier on new features\n",
    "        training_data = BetterFeatureExtractor(training_digit_input)\n",
    "        digit_classifier.train(training_data, training_digit_label)\n",
    "\n",
    "        # test_classifier on new features\n",
    "        test_data = BetterFeatureExtractor(test_digit_input)\n",
    "        new_feature_predictions = digit_classifier.predict(test_data)\n",
    "        new_feature_accuracy = np.count_nonzero(\n",
    "            new_feature_predictions == test_digit_label)/test_digit_label.shape[0]\n",
    "        print(f\"Accuracy on test data using new features is: {new_feature_accuracy}\")\n",
    "\n",
    "        # compare accuracies achieved on different sets of features\n",
    "        accuracy_gain = new_feature_accuracy - basic_feature_accuracy\n",
    "        print(f\"Accuracy gained by using new features is: {accuracy_gain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission test <a name=\"Submission-Test\"></a>\n",
    "The following cell tests if your notebook is ready for submission. **You must not skip this step!**\n",
    "\n",
    "Restart the kernel and run the entire notebook (Kernel → Restart & Run All). Now look at the output of the cell below.\n",
    "\n",
    "If there is no output, then your submission is not ready. Either your code is still running (did you forget to skip tests?) or it caused an error.\n",
    "\n",
    "As previously mentioned, failing to follow these instructions can result in a grade of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "863cf7467fd666c5600201aaffc44555",
     "grade": false,
     "grade_id": "cell-ce83a675162843d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:\n",
      "If you opt to submit the code for the second part of the assignment,\n",
      "please note that your code does not contain the variable digit_classifier.\n",
      "Therefore, our tests for part 2 are unable to run, but it would not affect your mark\n",
      "as long as your implementation of the BetterFeatureExtractor function is correct.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BetterFeatureExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5844\\38638997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mtest_digit_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/test_digit_input.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mtest_data_basic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasicFeatureExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_digit_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtest_data_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBetterFeatureExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_digit_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mfail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BetterFeatureExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_TESTS constant to True in the cell above.\")\n",
    "    \n",
    "p1 = pathlib.Path('./readme.txt')\n",
    "p2 = pathlib.Path('./readme.md')\n",
    "if not (p1.is_file() or p2.is_file()):\n",
    "    fail = True;\n",
    "    print(\"You must include a separate file called readme.txt or readme.md in your submission.\")\n",
    "    \n",
    "p3 = pathlib.Path('./classification.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"This notebook file must be named classification.ipynb\")\n",
    "    \n",
    "if \"my_accuracy_estimate\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"You must include a function called my_accuracy_estimate.\")\n",
    "else:\n",
    "    if my_accuracy_estimate() == 0.5:\n",
    "        print(\"Warning:\")\n",
    "        print(\"You do not seem to have provided an accuracy estimate, it is set to 0.5.\")\n",
    "        print(\"This is the actually the worst possible accuracy – if your classifier\")\n",
    "        print(\"got 0.1 then it could invert its results to get 0.9!\")\n",
    "    \n",
    "if \"spam_classifier\" not in locals():\n",
    "    fail = True\n",
    "    print(\"Your code should contain the variable spam_classifier.\")\n",
    "else:\n",
    "    if not hasattr(spam_classifier, 'predict'):\n",
    "        fail = True\n",
    "        print(\"The variable spam_classifier should have the method 'predict'.\")\n",
    "        \n",
    "if \"digit_classifier\" not in locals():\n",
    "    print(\"Warning:\")\n",
    "    print(\"If you opt to submit the code for the second part of the assignment,\")\n",
    "    print(\"please note that your code does not contain the variable digit_classifier.\")\n",
    "    print(\"Therefore, our tests for part 2 are unable to run, but it would not affect your mark\")\n",
    "    print(\"as long as your implementation of the BetterFeatureExtractor function is correct.\")\n",
    "else:\n",
    "    if (not hasattr(digit_classifier, 'train')) or \\\n",
    "        (not hasattr(digit_classifier, 'predict')):\n",
    "            print(\"Warning:\")\n",
    "            print(\"If you opt to submit the code for the second part of the assignment,\")\n",
    "            print(\"please note that the variable digit_classifier does not have both methods 'train' and 'predict'.\")\n",
    "            print(\"Therefore, our tests for part 2 are unable to run, but it would not affect your mark\")\n",
    "            print(\"as long as your implementation of the BetterFeatureExtractor function is correct.\")\n",
    "            \n",
    "test_digit_input = np.load(\"data/test_digit_input.npy\")\n",
    "test_data_basic = BasicFeatureExtractor(test_digit_input)\n",
    "test_data_new = BetterFeatureExtractor(test_digit_input)\n",
    "if not isinstance(test_data_new, np.ndarray):\n",
    "    fail = True\n",
    "    print(\"The BetterFeatureExtractor function should return a numpy array.\")\n",
    "if np.array_equal(test_data_basic, test_data_new):\n",
    "    print(\"Warning:\")\n",
    "    print(\"If you opt to submit the code for the second part of the assignment\")\n",
    "    print(\"you do not seem to implement the BetterFeatureExtractor function.\")\n",
    "    print(\"It returns the basic features.\")\n",
    "    \n",
    "if len(test_data_new.shape) != 2:\n",
    "    fail = True\n",
    "    print(\"The BetterFeatureExtractor function should return a 2-dimensional array.\")\n",
    "\n",
    "if test_data_new.shape[0] != 300:\n",
    "    fail = True\n",
    "    print(\"The BetterFeatureExtractor function should not alter the first dimension,\")\n",
    "    print(\"which should correspond to the number of data points in the data.\")\n",
    "    \n",
    "if not np.array_equal(np.unique(test_data_new), np.array([0, 1])):\n",
    "    fail = True\n",
    "    print(\"The BetterFeatureExtractor function should return only binary features.\")\n",
    "    \n",
    "if fail:\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print(\"All checks passed. When you are ready to submit, upload the notebook and readme file to the\")\n",
    "    print(\"assignment page, without changing any filenames.\")\n",
    "    print()\n",
    "    print(\"INFO: Make sure you include a readme file into your submission.\")\n",
    "    print(\"Failing to include this could result in an overall grade of zero.\")\n",
    "    print()\n",
    "    print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.).\")\n",
    "    print(\"Please note that you can submit up to 20 files without archiving them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "badbc892f539e03ad0acdb369f7e0993",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
